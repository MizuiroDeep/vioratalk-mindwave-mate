"""test_mock_llm_engine.py - MockLLMEngineãƒ†ã‚¹ãƒˆ

MockLLMEngineã®å˜ä½“ãƒ†ã‚¹ãƒˆã€‚
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ¢ãƒƒã‚¯å®Ÿè£…ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚

ãƒ†ã‚¹ãƒˆå®Ÿè£…ã‚¬ã‚¤ãƒ‰ v1.3æº–æ‹ 
ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ v1.7æº–æ‹ 
é–‹ç™ºè¦ç´„æ›¸ v1.12æº–æ‹ 
éåŒæœŸå‡¦ç†å®Ÿè£…ã‚¬ã‚¤ãƒ‰ v1.1æº–æ‹ 
ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æŒ‡é‡ v1.20æº–æ‹ 
"""

import asyncio
import time

import pytest

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from tests.mocks.mock_llm_engine import LLMResponse, Message, MockLLMEngine
from vioratalk.core.base import ComponentState
from vioratalk.core.exceptions import APIError, LLMError, ModelNotFoundError

# ============================================================================
# ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£
# ============================================================================


@pytest.fixture
async def mock_llm_engine():
    """MockLLMEngineã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    engine = MockLLMEngine()
    await engine.initialize()
    yield engine
    await engine.cleanup()


@pytest.fixture
def config_with_streaming():
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹ã®è¨­å®š"""
    return {"delay": 0.05, "streaming": True, "model": "mock-gpt-4", "max_tokens": 1000}


@pytest.fixture
def sample_messages():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ"""
    return [
        Message(role="user", content="ã“ã‚“ã«ã¡ã¯"),
        Message(role="assistant", content="ã“ã‚“ã«ã¡ã¯ï¼ç¢§è¡£ã§ã™ã€‚"),
        Message(role="user", content="ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ"),
    ]


# ============================================================================
# åˆæœŸåŒ–ãƒ»çµ‚äº†ãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.asyncio
@pytest.mark.unit
@pytest.mark.phase(3)
class TestInitializationAndCleanup:
    """åˆæœŸåŒ–ã¨çµ‚äº†å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ"""

    async def test_initialization_success(self):
        """æ­£å¸¸ãªåˆæœŸåŒ–ã®ç¢ºèª"""
        engine = MockLLMEngine()

        # åˆæœŸçŠ¶æ…‹ã®ç¢ºèª
        assert engine.state == ComponentState.NOT_INITIALIZED
        assert engine.current_model == "mock-gpt-3.5"
        assert engine.max_tokens == 2048

        # åˆæœŸåŒ–
        await engine.initialize()

        # åˆæœŸåŒ–å¾Œã®çŠ¶æ…‹ç¢ºèª
        assert engine.state == ComponentState.READY
        assert engine.response_delay == 0.1
        assert engine.error_mode is False
        assert engine.streaming_enabled is False

        await engine.cleanup()

    async def test_initialization_with_config(self, config_with_streaming):
        """è¨­å®šä»˜ãåˆæœŸåŒ–ã®ç¢ºèª"""
        engine = MockLLMEngine(config=config_with_streaming)

        # è¨­å®šãŒåæ˜ ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        assert engine.response_delay == 0.05
        assert engine.streaming_enabled is True
        assert engine.current_model == "mock-gpt-4"
        assert engine.max_tokens == 1000

        await engine.initialize()
        assert engine.state == ComponentState.READY

        await engine.cleanup()

    async def test_cleanup(self, mock_llm_engine):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ã®ç¢ºèª"""
        # åˆæœŸåŒ–æ¸ˆã¿ã®çŠ¶æ…‹ã‹ã‚‰
        assert mock_llm_engine.state == ComponentState.READY

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        await mock_llm_engine.cleanup()

        # çµ‚äº†çŠ¶æ…‹ã®ç¢ºèª
        assert mock_llm_engine.state == ComponentState.TERMINATED


# ============================================================================
# generateãƒ¡ã‚½ãƒƒãƒ‰ãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.asyncio
@pytest.mark.unit
@pytest.mark.phase(3)
class TestGenerate:
    """generateãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ†ã‚¹ãƒˆ"""

    async def test_generate_simple_prompt(self, mock_llm_engine):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®ç”Ÿæˆ"""
        response = await mock_llm_engine.generate("ã“ã‚“ã«ã¡ã¯")

        assert isinstance(response, LLMResponse)
        assert len(response.content) > 0
        assert response.model == "mock-gpt-3.5"
        assert response.finish_reason == "stop"
        assert "total_tokens" in response.usage

    async def test_generate_with_system_prompt(self, mock_llm_engine):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãç”Ÿæˆ"""
        response = await mock_llm_engine.generate(prompt="è³ªå•ãŒã‚ã‚Šã¾ã™", system_prompt="ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™")

        assert response.content is not None
        assert response.metadata["response_type"] == "default"  # "è³ªå•"ã ã‘ã§ã¯è³ªå•ã¨ã—ã¦åˆ¤å®šã•ã‚Œãªã„

    async def test_generate_with_temperature(self, mock_llm_engine):
        """æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ãç”Ÿæˆ"""
        response = await mock_llm_engine.generate(prompt="å‰µé€ çš„ãªè©±ã‚’ã—ã¦", temperature=0.9)

        assert response.content is not None
        assert response.metadata["temperature"] == 0.9

    async def test_generate_with_max_tokens(self, mock_llm_engine):
        """æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°æŒ‡å®š"""
        response = await mock_llm_engine.generate(prompt="é•·ã„èª¬æ˜ã‚’ã—ã¦", max_tokens=100)

        assert response.content is not None
        assert response.usage["total_tokens"] <= 100

    async def test_generate_not_ready_state(self):
        """åˆæœŸåŒ–å‰ã®generateå‘¼ã³å‡ºã—"""
        engine = MockLLMEngine()

        # åˆæœŸåŒ–ã›ãšã«generateã‚’å‘¼ã³å‡ºã—
        with pytest.raises(LLMError) as exc_info:
            await engine.generate("test")

        assert exc_info.value.error_code == "E2000"
        assert "not ready" in str(exc_info.value)

    async def test_generate_error_mode(self, mock_llm_engine):
        """ã‚¨ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰æ™‚ã®å‹•ä½œç¢ºèª"""
        # ã‚¨ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
        mock_llm_engine.set_error_mode(True)

        with pytest.raises(APIError) as exc_info:
            await mock_llm_engine.generate("test")

        assert exc_info.value.error_code == "E2001"
        assert "Mock API error" in str(exc_info.value)


# ============================================================================
# ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ¥å¿œç­”ãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.asyncio
@pytest.mark.unit
@pytest.mark.phase(3)
class TestCharacterResponses:
    """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ¥å¿œç­”ã®ãƒ†ã‚¹ãƒˆ"""

    async def test_aoi_responses(self, mock_llm_engine):
        """ç¢§è¡£ï¼ˆ001_aoiï¼‰ã®å¿œç­”ãƒ†ã‚¹ãƒˆ"""
        # æŒ¨æ‹¶
        response = await mock_llm_engine.generate(
            prompt="ã“ã‚“ã«ã¡ã¯", system_prompt="character_id:001_aoi"
        )
        assert "ç¢§è¡£" in response.content
        assert response.metadata["character_id"] == "001_aoi"

        # è³ªå•
        response = await mock_llm_engine.generate(
            prompt="ã“ã‚Œã¯ä½•ã§ã™ã‹ï¼Ÿ", system_prompt="character_id:001_aoi"
        )
        assert "è³ªå•" in response.content or "èˆˆå‘³æ·±ã„" in response.content

        # ã‚³ãƒãƒ³ãƒ‰
        response = await mock_llm_engine.generate(
            prompt="éŸ³æ¥½ã‚’å†ç”Ÿã—ã¦ãã ã•ã„", system_prompt="character_id:001_aoi"
        )
        assert "æ‰¿çŸ¥" in response.content or "å¯¾å¿œ" in response.content

    async def test_haru_responses(self, mock_llm_engine):
        """æ˜¥äººï¼ˆ002_haruï¼‰ã®å¿œç­”ãƒ†ã‚¹ãƒˆ"""
        # æŒ¨æ‹¶
        response = await mock_llm_engine.generate(
            prompt="ã‚„ã‚", system_prompt="character_id:002_haru"
        )
        assert "æ˜¥äºº" in response.content or "ã‚„ã‚" in response.content
        assert response.metadata["character_id"] == "002_haru"

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¿œç­”
        response = await mock_llm_engine.generate(
            prompt="ä»Šæ—¥ã¯å¿™ã—ã‹ã£ãŸ", system_prompt="character_id:002_haru"
        )
        assert "ãã†ãªã‚“ã " in response.content or "ãŠã‚‚ã—ã‚ã„" in response.content

    async def test_yui_responses(self, mock_llm_engine):
        """çµè¡£ï¼ˆ003_yuiï¼‰ã®å¿œç­”ãƒ†ã‚¹ãƒˆ"""
        # æŒ¨æ‹¶
        response = await mock_llm_engine.generate(
            prompt="åˆã‚ã¾ã—ã¦", system_prompt="character_id:003_yui"
        )
        # ã€Œåˆã‚ã¾ã—ã¦ã€ã¯æŒ¨æ‹¶ã¨ã—ã¦åˆ¤å®šã•ã‚Œãªã„ã®ã§defaultå¿œç­”
        assert response.metadata["character_id"] == "003_yui"

        # ã‚³ãƒãƒ³ãƒ‰å½¢å¼ï¼ˆæ˜ç¢ºã«ã‚³ãƒãƒ³ãƒ‰ã¨ã—ã¦åˆ¤å®šã•ã‚Œã‚‹ã‚‚ã®ï¼‰
        response = await mock_llm_engine.generate(
            prompt="ã“ã‚Œã‚’ã‚„ã£ã¦ãã ã•ã„", system_prompt="character_id:003_yui"
        )
        assert "ã‚„ã£ã¦ã¿ã¾ã™ã­" in response.content or "ã¯ã„" in response.content

    async def test_unknown_character(self, mock_llm_engine):
        """æœªçŸ¥ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å¿œç­”ãƒ†ã‚¹ãƒˆ"""
        response = await mock_llm_engine.generate(
            prompt="ã“ã‚“ã«ã¡ã¯", system_prompt="character_id:999_unknown"
        )

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å¿œç­”ãŒè¿”ã•ã‚Œã‚‹
        assert response.metadata["character_id"] == "default"
        assert "ã“ã‚“ã«ã¡ã¯" in response.content or "ãŠæ‰‹ä¼ã„" in response.content


# ============================================================================
# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.asyncio
@pytest.mark.unit
@pytest.mark.phase(3)
class TestStreamGenerate:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ"""

    async def test_stream_generate_success(self, mock_llm_engine):
        """æ­£å¸¸ãªã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ"""
        mock_llm_engine.streaming_enabled = True

        chunks = []
        async for chunk in mock_llm_engine.stream_generate("ã“ã‚“ã«ã¡ã¯"):
            chunks.append(chunk)

        full_response = "".join(chunks)
        assert len(full_response) > 0
        assert len(chunks) == len(full_response)  # 1æ–‡å­—ãšã¤ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°

    async def test_stream_generate_character_based(self, mock_llm_engine):
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ¥ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ"""
        mock_llm_engine.streaming_enabled = True
        mock_llm_engine.set_custom_response("test", "ABC")

        chunks = []
        async for chunk in mock_llm_engine.stream_generate("test"):
            chunks.append(chunk)

        assert chunks == ["A", "B", "C"]

    async def test_stream_generate_with_delay(self, mock_llm_engine):
        """é…å»¶ä»˜ãã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ"""
        mock_llm_engine.streaming_enabled = True
        mock_llm_engine.set_custom_response("test", "12345")

        start_time = time.time()
        chunks = []

        async for chunk in mock_llm_engine.stream_generate("test"):
            chunks.append(chunk)

        elapsed_time = time.time() - start_time

        # å„æ–‡å­—ã«0.01ç§’ã®é…å»¶ãŒã‚ã‚‹ã®ã§ã€5æ–‡å­—ã§ç´„0.05ç§’ä»¥ä¸Š
        assert elapsed_time >= 0.04  # èª¤å·®ã‚’è€ƒæ…®
        assert len(chunks) == 5

    async def test_stream_generate_error_mode(self, mock_llm_engine):
        """ã‚¨ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°"""
        mock_llm_engine.streaming_enabled = True
        mock_llm_engine.set_error_mode(True)

        with pytest.raises(APIError) as exc_info:
            async for _ in mock_llm_engine.stream_generate("test"):
                pass

        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æŒ‡é‡ v1.20æº–æ‹ : E2001ï¼ˆå¿œç­”ç”Ÿæˆå¤±æ•—ï¼‰
        assert exc_info.value.error_code == "E2001"
        assert "Mock API error" in str(exc_info.value)

    async def test_stream_generate_with_system_prompt(self, mock_llm_engine):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°"""
        mock_llm_engine.streaming_enabled = True

        chunks = []
        async for chunk in mock_llm_engine.stream_generate(
            prompt="ã“ã‚“ã«ã¡ã¯", system_prompt="character_id:002_haru"
        ):
            chunks.append(chunk)

        full_response = "".join(chunks)
        # æ˜¥äººã®å¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå«ã¾ã‚Œã‚‹
        assert "æ˜¥äºº" in full_response or "ã‚„ã‚" in full_response


# ============================================================================
# ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.unit
@pytest.mark.phase(3)
class TestModelManagement:
    """ãƒ¢ãƒ‡ãƒ«ç®¡ç†æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""

    def test_get_available_models(self, mock_llm_engine):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã®å–å¾—"""
        models = mock_llm_engine.get_available_models()

        assert isinstance(models, list)
        assert len(models) == 4
        assert "mock-gpt-3.5" in models
        assert "mock-gpt-4" in models
        assert "mock-claude-3" in models
        assert "mock-gemini-pro" in models

    def test_set_model_valid(self, mock_llm_engine):
        """æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ã®è¨­å®š"""
        # åˆæœŸãƒ¢ãƒ‡ãƒ«ç¢ºèª
        assert mock_llm_engine.current_model == "mock-gpt-3.5"

        # ãƒ¢ãƒ‡ãƒ«å¤‰æ›´
        mock_llm_engine.set_model("mock-gpt-4")
        assert mock_llm_engine.current_model == "mock-gpt-4"

        # åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´
        mock_llm_engine.set_model("mock-claude-3")
        assert mock_llm_engine.current_model == "mock-claude-3"

    def test_set_model_invalid(self, mock_llm_engine):
        """ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«ã®è¨­å®š"""
        with pytest.raises(ModelNotFoundError) as exc_info:
            mock_llm_engine.set_model("invalid-model")

        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æŒ‡é‡ v1.20æº–æ‹ : E2404ï¼ˆãƒ¢ãƒ‡ãƒ«é¸æŠãƒ­ã‚¸ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ï¼‰
        assert exc_info.value.error_code == "E2404"
        assert "Model 'invalid-model' not found" in str(exc_info.value)

        # ãƒ¢ãƒ‡ãƒ«ã¯å¤‰æ›´ã•ã‚Œã¦ã„ãªã„
        assert mock_llm_engine.current_model == "mock-gpt-3.5"


# ============================================================================
# ä¼šè©±å±¥æ­´ç®¡ç†ãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.asyncio
@pytest.mark.unit
@pytest.mark.phase(3)
class TestConversationHistory:
    """ä¼šè©±å±¥æ­´ç®¡ç†ã®ãƒ†ã‚¹ãƒˆ"""

    async def test_add_message(self, mock_llm_engine):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ ã®ãƒ†ã‚¹ãƒˆ"""
        # åˆæœŸçŠ¶æ…‹
        assert len(mock_llm_engine.conversation_history) == 0

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
        mock_llm_engine.add_message("user", "ã“ã‚“ã«ã¡ã¯")
        assert len(mock_llm_engine.conversation_history) == 1
        assert mock_llm_engine.conversation_history[0].role == "user"
        assert mock_llm_engine.conversation_history[0].content == "ã“ã‚“ã«ã¡ã¯"

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
        mock_llm_engine.add_message("assistant", "ã“ã‚“ã«ã¡ã¯ï¼")
        assert len(mock_llm_engine.conversation_history) == 2

    async def test_clear_history(self, mock_llm_engine):
        """å±¥æ­´ã‚¯ãƒªã‚¢ã®ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        mock_llm_engine.add_message("user", "test1")
        mock_llm_engine.add_message("assistant", "response1")
        mock_llm_engine.add_message("user", "test2")

        assert len(mock_llm_engine.conversation_history) == 3

        # å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
        mock_llm_engine.clear_history()
        assert len(mock_llm_engine.conversation_history) == 0

    async def test_get_history(self, mock_llm_engine):
        """å±¥æ­´å–å¾—ã®ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        mock_llm_engine.add_message("user", "è³ªå•1")
        mock_llm_engine.add_message("assistant", "å›ç­”1")

        history = mock_llm_engine.get_history()

        assert len(history) == 2
        assert history[0].role == "user"
        assert history[0].content == "è³ªå•1"
        assert history[1].role == "assistant"
        assert history[1].content == "å›ç­”1"

        # å–å¾—ã—ãŸå±¥æ­´ã¯ç‹¬ç«‹ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        history.append(Message(role="system", content="test"))
        assert len(mock_llm_engine.conversation_history) == 2  # å…ƒã®å±¥æ­´ã¯å¤‰æ›´ã•ã‚Œãªã„

    async def test_history_with_generation(self, mock_llm_engine):
        """ç”Ÿæˆæ™‚ã®å±¥æ­´è¿½åŠ ãƒ†ã‚¹ãƒˆ"""
        # åˆæœŸçŠ¶æ…‹
        assert len(mock_llm_engine.conversation_history) == 0

        # ç”Ÿæˆå®Ÿè¡Œ
        await mock_llm_engine.generate("è³ªå•ã§ã™")

        # å±¥æ­´ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert len(mock_llm_engine.conversation_history) == 2
        assert mock_llm_engine.conversation_history[0].role == "user"
        assert mock_llm_engine.conversation_history[0].content == "è³ªå•ã§ã™"
        assert mock_llm_engine.conversation_history[1].role == "assistant"


# ============================================================================
# ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¡ã‚½ãƒƒãƒ‰ãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.asyncio
@pytest.mark.unit
@pytest.mark.phase(3)
class TestUtilityMethods:
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ†ã‚¹ãƒˆ"""

    async def test_set_error_mode(self, mock_llm_engine):
        """ã‚¨ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰è¨­å®šã®ç¢ºèª"""
        # åˆæœŸçŠ¶æ…‹
        assert mock_llm_engine.error_mode is False

        # ã‚¨ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–
        mock_llm_engine.set_error_mode(True)
        assert mock_llm_engine.error_mode is True

        # ã‚¨ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰ç„¡åŠ¹åŒ–
        mock_llm_engine.set_error_mode(False)
        assert mock_llm_engine.error_mode is False

    async def test_set_custom_response(self, mock_llm_engine):
        """ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¨­å®šã®ç¢ºèª"""
        custom_text = "ã‚«ã‚¹ã‚¿ãƒ å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ"
        mock_llm_engine.set_custom_response("test_prompt", custom_text)

        # ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        response = await mock_llm_engine.generate("test_prompt")
        assert response.content == custom_text

    async def test_custom_response_in_generation(self, mock_llm_engine):
        """ç”Ÿæˆæ™‚ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½¿ç”¨"""
        custom_text = "ç‰¹åˆ¥ãªå¿œç­”"
        mock_llm_engine.set_custom_response("ç‰¹åˆ¥ãªè³ªå•", custom_text)

        # é€šå¸¸ã®å¿œç­”
        normal_response = await mock_llm_engine.generate("é€šå¸¸ã®è³ªå•")
        assert normal_response.content != custom_text

        # ã‚«ã‚¹ã‚¿ãƒ å¿œç­”
        custom_response = await mock_llm_engine.generate("ç‰¹åˆ¥ãªè³ªå•")
        assert custom_response.content == custom_text

    async def test_set_response_delay(self, mock_llm_engine):
        """å¿œç­”é…å»¶è¨­å®šã®ç¢ºèª"""
        # åˆæœŸå€¤ç¢ºèª
        assert mock_llm_engine.response_delay == 0.1

        # é…å»¶è¨­å®š
        mock_llm_engine.set_response_delay(0.5)
        assert mock_llm_engine.response_delay == 0.5

        # è² ã®å€¤ã¯0ã«ã‚¯ãƒªãƒƒãƒ—
        mock_llm_engine.set_response_delay(-1.0)
        assert mock_llm_engine.response_delay == 0.0

    async def test_get_statistics(self, mock_llm_engine):
        """çµ±è¨ˆæƒ…å ±å–å¾—ã®ç¢ºèª"""
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¨­å®š
        mock_llm_engine.set_custom_response("test1", "response1")
        mock_llm_engine.set_custom_response("test2", "response2")

        # ç”Ÿæˆã‚’å®Ÿè¡Œ
        await mock_llm_engine.generate("test1")
        await mock_llm_engine.generate("test2")

        stats = mock_llm_engine.get_statistics()

        assert stats["total_requests"] == 2
        assert stats["total_tokens"] > 0
        assert stats["average_tokens_per_request"] > 0
        assert stats["conversation_length"] == 4  # 2ã¤ã®ç”Ÿæˆã§4ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸


# ============================================================================
# éåŒæœŸå‡¦ç†ãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.asyncio
@pytest.mark.unit
@pytest.mark.phase(3)
class TestAsyncBehavior:
    """éåŒæœŸå‡¦ç†ã®å‹•ä½œãƒ†ã‚¹ãƒˆ"""

    async def test_response_delay(self, mock_llm_engine):
        """å¿œç­”é…å»¶ã®ãƒ†ã‚¹ãƒˆ"""
        # é…å»¶ã‚’0.2ç§’ã«è¨­å®š
        mock_llm_engine.set_response_delay(0.2)

        # å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬
        start_time = time.time()

        response = await mock_llm_engine.generate("test")

        elapsed_time = time.time() - start_time

        # é…å»¶ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆèª¤å·®ã‚’è€ƒæ…®ï¼‰
        assert elapsed_time >= 0.2
        assert elapsed_time < 0.3
        assert response.content is not None

    async def test_concurrent_generations(self, mock_llm_engine):
        """ä¸¦è¡Œç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ"""
        # è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
        prompts = ["è³ªå•1", "è³ªå•2", "è³ªå•3"]

        # ä¸¦è¡Œå®Ÿè¡Œ
        tasks = [mock_llm_engine.generate(prompt) for prompt in prompts]
        responses = await asyncio.gather(*tasks)

        # ã™ã¹ã¦æˆåŠŸã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert len(responses) == 3
        for response in responses:
            assert isinstance(response, LLMResponse)
            assert response.content is not None


# ============================================================================
# ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.asyncio
@pytest.mark.unit
@pytest.mark.phase(3)
class TestEdgeCases:
    """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""

    async def test_empty_prompt(self, mock_llm_engine):
        """ç©ºã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        response = await mock_llm_engine.generate("")

        assert response.content is not None
        assert len(response.content) > 0

    async def test_very_long_prompt(self, mock_llm_engine):
        """éå¸¸ã«é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        long_prompt = "ã“ã‚Œã¯" * 1000  # 3000æ–‡å­—
        response = await mock_llm_engine.generate(long_prompt)

        assert response.content is not None
        assert response.usage["prompt_tokens"] > 100

    async def test_special_characters_in_prompt(self, mock_llm_engine):
        """ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        special_prompt = "ã“ã‚Œã¯\næ”¹è¡Œã¨\tã‚¿ãƒ–ã¨ğŸ˜€çµµæ–‡å­—ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã™ï¼"
        response = await mock_llm_engine.generate(special_prompt)

        assert response.content is not None
        assert len(response.content) > 0

    async def test_max_tokens_exceeded(self, mock_llm_engine):
        """æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¶…éã®ãƒ†ã‚¹ãƒˆ"""
        response = await mock_llm_engine.generate(prompt="çŸ­ã„", max_tokens=1)

        # finish_reasonãŒlengthã«ãªã‚‹
        assert response.finish_reason == "length"
        assert response.usage["completion_tokens"] <= 1


# ============================================================================
# ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—ãƒ†ã‚¹ãƒˆ
# ============================================================================


@pytest.mark.asyncio
@pytest.mark.unit
@pytest.mark.phase(3)
class TestTokenCalculation:
    """ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ"""

    async def test_token_usage_calculation(self, mock_llm_engine):
        """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è¨ˆç®—ã®ç¢ºèª"""
        response = await mock_llm_engine.generate("ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™")

        usage = response.usage
        assert "prompt_tokens" in usage
        assert "completion_tokens" in usage
        assert "total_tokens" in usage

        # åˆè¨ˆãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèª
        assert usage["total_tokens"] == usage["prompt_tokens"] + usage["completion_tokens"]

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¦¥å½“ãªç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert usage["prompt_tokens"] > 0
        assert usage["completion_tokens"] > 0

    async def test_token_approximation(self, mock_llm_engine):
        """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¿‘ä¼¼è¨ˆç®—ç¢ºèª"""
        # æ—¥æœ¬èªã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ1æ–‡å­—â‰’2ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        japanese_prompt = "ã“ã‚“ã«ã¡ã¯"  # 5æ–‡å­—
        response = await mock_llm_engine.generate(japanese_prompt)

        # æ—¥æœ¬èªã¯1æ–‡å­—2ãƒˆãƒ¼ã‚¯ãƒ³ã§è¨ˆç®—ã•ã‚Œã‚‹ãŸã‚ã€ç´„10ãƒˆãƒ¼ã‚¯ãƒ³
        assert response.usage["prompt_tokens"] >= 8  # èª¤å·®ã‚’è€ƒæ…®
        assert response.usage["prompt_tokens"] <= 12
